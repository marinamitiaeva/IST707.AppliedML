{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "135c4074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/codespace/.python/current/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: pandas in /home/codespace/.local/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in /home/codespace/.local/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: seaborn in /home/codespace/.local/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy in /home/codespace/.local/lib/python3.12/site-packages (2.1.1)\n",
      "Requirement already satisfied: scikit-learn in /home/codespace/.local/lib/python3.12/site-packages (1.5.2)\n",
      "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.12/site-packages (1.4.2)\n",
      "Requirement already satisfied: click in /home/codespace/.python/current/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/codespace/.python/current/lib/python3.12/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /home/codespace/.python/current/lib/python3.12/site-packages (from nltk) (4.66.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install necessary packages\n",
    "%pip install nltk pandas matplotlib seaborn numpy scikit-learn joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62a22668",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29306945",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests = pd.read_csv('/workspaces/IST707.Pothole-Prediction-SYR/data/raw/SYRCityline_Requests_(2021-Present).csv')\n",
    "weather = pd.read_csv('/workspaces/IST707.Pothole-Prediction-SYR/data/raw/40_Year_Temperature_Dataset_SU-2.csv')\n",
    "ratings_2021 = pd.read_csv('/workspaces/IST707.Pothole-Prediction-SYR/data/raw/Pavement_Ratings_(2021)-2.csv')\n",
    "ratings_2022 = pd.read_csv('/workspaces/IST707.Pothole-Prediction-SYR/data/raw/Syracuse_Pavement_Ratings_(2022).csv')\n",
    "ratings_2023 = pd.read_csv('/workspaces/IST707.Pothole-Prediction-SYR/data/raw/Syracuse_Ratings_Open_Data_Copy_2023_8469753568545483898.csv')\n",
    "streets = pd.read_csv('/workspaces/IST707.Pothole-Prediction-SYR/data/raw/City_Streets_2011.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f375cf2d",
   "metadata": {},
   "source": [
    "Approach:\n",
    "1) Load the request data, filter out the pothole requests from 2021-2023\n",
    "2) Standardize the address in potholes and match with the street dictionary \n",
    "3) Concatenate the rating data\n",
    "4) Filter out temperature data\n",
    "5) Merge  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0747188b",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests['Description'] = requests['Description'].astype(str)\n",
    "potholes = requests[requests['Category'] == 'Potholes']\n",
    "# Ensure the 'Date' column is in datetime format\n",
    "potholes = potholes[['Rating', 'Address', 'Description', 'Created_at_local', 'Minutes_to_closed']]\n",
    "potholes['Date'] = pd.to_datetime(potholes['Created_at_local'], format='%m/%d/%Y - %I:%M%p').dt.date\n",
    "# Define the date range\n",
    "start_date = pd.to_datetime(\"2021-01-01\").date()\n",
    "end_date = pd.to_datetime(\"2023-12-31\").date()\n",
    "# Filter the DataFrame to the specified range\n",
    "filtered_potholes = potholes[(potholes['Date'] >= start_date) & (potholes['Date'] <= end_date)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73052970",
   "metadata": {},
   "source": [
    "Clean up the street names column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d7cce9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5171/3488095503.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_potholes['Street'] = filtered_potholes['Address'].apply(extract_clean_street_name)\n",
      "/tmp/ipykernel_5171/3488095503.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_potholes['Street'] = filtered_potholes['Street'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "# Function to extract cleaner street names\n",
    "def extract_clean_street_name(address):\n",
    "    # More strict extraction to focus on just the name and type\n",
    "    match = re.search(r'\\b([A-Za-z]+)\\s*(St|Street|Ave|Avenue|Blvd|Boulevard|Dr|Drive|Rd|Road|Ln|Lane|Way|Circle|Cir)\\b', address, re.IGNORECASE)\n",
    "    if match:\n",
    "        # Normalize the street name by removing common suffixes\n",
    "        street_name = match.group(1).strip().upper()  # Extract and normalize the base name\n",
    "        return street_name\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Apply the revised function to the Address column\n",
    "filtered_potholes['Street'] = filtered_potholes['Address'].apply(extract_clean_street_name)\n",
    "filtered_potholes['Street'] = filtered_potholes['Street'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a891e7",
   "metadata": {},
   "source": [
    "Caclulate sentiment analysis of potholes in relation to the street names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66963867",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5171/4084124416.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_potholes['SentimentScore'] = filtered_potholes['Description'].apply(sentiment_score)\n",
      "/tmp/ipykernel_5171/4084124416.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_potholes['SeverityLevel'] = filtered_potholes['SentimentScore'].apply(categorize_severity)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Street</th>\n",
       "      <th>SentimentScore</th>\n",
       "      <th>SeverityLevel</th>\n",
       "      <th>Potholes_Count</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Minutes_to_closed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-06-15</td>\n",
       "      <td>MCBRIDE</td>\n",
       "      <td>-0.8054</td>\n",
       "      <td>high</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>32692.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-06-15</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>medium</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>32918.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-06-15</td>\n",
       "      <td>SOLAR</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>medium</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10439.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-06-22</td>\n",
       "      <td>CARBON</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>medium</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>32915.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-06-22</td>\n",
       "      <td>COLVIN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>medium</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>31500.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date   Street  SentimentScore SeverityLevel  Potholes_Count  Rating  \\\n",
       "0  2021-06-15  MCBRIDE         -0.8054          high               2     2.0   \n",
       "1  2021-06-15     None          0.0000        medium               1     2.0   \n",
       "2  2021-06-15    SOLAR          0.0000        medium               1     2.0   \n",
       "3  2021-06-22   CARBON          0.0000        medium               1     2.0   \n",
       "4  2021-06-22   COLVIN          0.0000        medium               1     2.0   \n",
       "\n",
       "   Minutes_to_closed  \n",
       "0            32692.0  \n",
       "1            32918.0  \n",
       "2            10439.0  \n",
       "3            32915.0  \n",
       "4            31500.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentiment_score(description):\n",
    "    # Tokenize the description\n",
    "    tokens = word_tokenize(description)\n",
    "    \n",
    "    # Join the tokens back into a string (optional step, depending on your approach)\n",
    "    text = ' '.join(tokens)\n",
    "    \n",
    "    # Calculate sentiment score\n",
    "    score = sia.polarity_scores(text)['compound']\n",
    "    \n",
    "    # Return the compound sentiment score\n",
    "    return score\n",
    "\n",
    "filtered_potholes['SentimentScore'] = filtered_potholes['Description'].apply(sentiment_score)\n",
    "\n",
    "def categorize_severity(score):\n",
    "    if score <= -0.5:\n",
    "        return 'high'\n",
    "    elif score <= 0:\n",
    "        return 'medium'\n",
    "    else:\n",
    "        return 'low'\n",
    "\n",
    "filtered_potholes['SeverityLevel'] = filtered_potholes['SentimentScore'].apply(categorize_severity)\n",
    "\n",
    "potholes_agg = filtered_potholes.groupby(['Date', 'Street']).agg({\n",
    "    'SentimentScore': 'mean',  # Average sentiment score per day\n",
    "    'SeverityLevel': lambda x: x.mode()[0] if not x.empty else None,  # Most common severity level per day\n",
    "    'Description': 'count',  # Count the number of reports per day\n",
    "    'Rating': 'mean',  # Average rating\n",
    "    'Minutes_to_closed': 'mean'  # Average minutes to close\n",
    "}).rename(columns={'Description': 'Potholes_Count'}).reset_index()\n",
    "\n",
    "# potholes_agg['Date'] = pd.to_datetime(potholes_agg['Date'])\n",
    "potholes_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd15ba89",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_2021['Year'] = 2021\n",
    "ratings_2022['Year'] = 2022\n",
    "ratings_2023['Year'] = 2023\n",
    "\n",
    "ratings_2021.columns = ratings_2022.columns = ratings_2023.columns\n",
    "# Concatenate the data\n",
    "combined_data = pd.concat([ratings_2021, ratings_2022, ratings_2023], ignore_index=True)\n",
    "combined_data = combined_data.rename(columns={'Rating2023': 'Pavement_rating'})\n",
    "\n",
    "agg_rating = combined_data.groupby(['Year', 'STREET_NAM']).agg({\n",
    "    'Pavement_rating': 'mean',  # Assuming you want the mean rating; adjust the aggregation as needed\n",
    "    'Miles': 'sum',  # Total miles\n",
    "    'Shape__Length': 'sum'  # Total shape length\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ad25da",
   "metadata": {},
   "source": [
    "Extract the street names and aggregate ratings, miles and shapes to each street"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6c5cd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique street names from the 'NAME' column\n",
    "streets_unique = streets['NAME'].unique()\n",
    "streets_unique\n",
    "\n",
    "agg_rating['STREET_NAM'] = agg_rating['STREET_NAM'].str.upper()\n",
    "\n",
    "# Function to find a partial match in streets_unique\n",
    "def find_partial_match(street_name):\n",
    "    for unique_name in streets_unique:\n",
    "        if unique_name in street_name:\n",
    "            return unique_name\n",
    "    return np.nan  # or return street_name if you want to keep original for no matches\n",
    "\n",
    "# Apply the matching function\n",
    "agg_rating['Matched_Street'] = agg_rating['STREET_NAM'].apply(find_partial_match)\n",
    "\n",
    "aggregated_rating_st = agg_rating.groupby(['Matched_Street', 'Year']).agg({\n",
    "    'Pavement_rating': 'mean',  # Average rating per matched street\n",
    "    'Miles': 'sum',  # Total miles per matched street\n",
    "    'Shape__Length': 'sum'  # Total shape length\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0cc7e1",
   "metadata": {},
   "source": [
    "More cleanup and aggregation of the aggregated data set based on street names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06712290",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5171/3595299375.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_merged_df.drop(columns=['Normalized_Street_x', 'Normalized_Street_y'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "potholes_agg['Year'] = pd.to_datetime(potholes_agg['Date']).dt.year.astype('int64')\n",
    "\n",
    "# Function to normalize street names (remove common suffixes and convert to uppercase)\n",
    "def normalize_street_name(street_name):\n",
    "    suffixes = r'(STREET|ST|AVENUE|AVE|BOULEVARD|BLVD|DRIVE|DR|ROAD|RD|LANE|LN|WAY|CIRCLE|CIR)$'\n",
    "    return pd.Series(street_name).replace(suffixes, '', regex=True).str.strip().str.upper()\n",
    "\n",
    "# Normalize street names in both dataframes\n",
    "aggregated_rating_st['Normalized_Street'] = normalize_street_name(aggregated_rating_st['Matched_Street'])\n",
    "potholes_agg['Normalized_Street'] = normalize_street_name(potholes_agg['Street'])\n",
    "\n",
    "# Perform the join based on the year\n",
    "merged_df = pd.merge(aggregated_rating_st, potholes_agg, left_on='Year', right_on='Year', how='left')\n",
    "\n",
    "# Define a function to check for partial matches\n",
    "def is_partial_match(street_name_agg, street_name_potholes):\n",
    "    if pd.isna(street_name_agg) or pd.isna(street_name_potholes):\n",
    "        return False\n",
    "    return street_name_agg in street_name_potholes or street_name_potholes in street_name_agg\n",
    "\n",
    "# Filter the merged dataframe for partial matches of street names\n",
    "filtered_merged_df = merged_df[merged_df.apply(lambda x: is_partial_match(x['Normalized_Street_x'], x['Normalized_Street_y']), axis=1)]\n",
    "\n",
    "# Drop the columns used for matching\n",
    "filtered_merged_df.drop(columns=['Normalized_Street_x', 'Normalized_Street_y'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0781e79",
   "metadata": {},
   "source": [
    "Complete the date cleanup on the new weather data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd06ab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Year', 'Month', and 'Day' columns to a single datetime column\n",
    "weather['Date'] = pd.to_datetime(weather[['Year', 'Month', 'Day']]).dt.date\n",
    "weather = weather[weather['Year'] >= 2021]\n",
    "weather = weather.drop(['Year', 'Month', 'Day'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4447581e",
   "metadata": {},
   "source": [
    "Merge the weather and existing aggregated data frame on dates, and then drop the year and street columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b0e829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.merge(weather, filtered_merged_df, on='Date', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "813f2e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merged_data.drop(['Year', 'Street'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bd8b93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.to_csv('/workspaces/IST707.Pothole-Prediction-SYR/data/processed/final_data.csv')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
